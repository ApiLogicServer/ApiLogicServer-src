---
title: LogicBank Probabilistic Rules API (AI Value Computation)
description: Training document for translating natural language into probabilistic value computation rules
source: Generic training for ApiLogicServer projects with probabilistic rules
usage: AI assistants read this to generate probabilistic + deterministic rules implementations
version: 2.4
prerequisites:
  - docs/training/genai_logic_patterns.md (CRITICAL import patterns, auto-discovery fixes)
  - docs/training/logic_bank_patterns.prompt (general patterns - event signatures, logging, request pattern)
  - docs/training/logic_bank_api.prompt (deterministic rule APIs)
changelog:
  - 2.4 (Nov 16, 2025): CLARIFIED - Circular import warning applies only to main declare_logic.py, discovery files are safe with module-level imports
  - 2.3 (Nov 16, 2025): Clarified auto_discovery.py is AUTO-GENERATED (don't modify, already correct)
---

# PREREQUISITES - READ THESE FIRST

Before using this file, you MUST read these foundation documents in order:

1. **docs/training/genai_logic_patterns.md** - CRITICAL patterns learned from demo prep
   - ‚ö†Ô∏è Circular import avoidance (applies to main logic/declare_logic.py ONLY, not discovery files)
   - ‚ö†Ô∏è Path resolution with .resolve() for YAML files
   - LogicBank triggered insert pattern
   - Common anti-patterns that cause server startup failures
   - Note: auto_discovery.py is auto-generated and already correct (don't modify)

2. **docs/training/logic_bank_patterns.prompt** - General LogicBank patterns
   - Event handler signatures (row, old_row, logic_row)
   - Logging with logic_row.log()
   - Request Pattern with new_logic_row()
   - Rule API syntax reference

3. **docs/training/logic_bank_api.prompt** - Deterministic rule APIs
   - Rule.sum(), Rule.count(), Rule.formula(), Rule.constraint(), etc.
   - Complete API signatures and examples

This file contains ONLY AI-specific probabilistic patterns.

=============================================================================
üö® CRITICAL FIXES FROM DEMO PREP (Nov 16, 2025)
=============================================================================

These errors occurred during demo prep testing and MUST be avoided:

**ERROR 1: Circular Import - "Session is already flushing" (applies to main declare_logic.py ONLY)**
‚ùå Problem: Importing LogicBank at module level IN MAIN logic/declare_logic.py
```python
# In logic/declare_logic.py (main file)
from logic_bank.logic_bank import Rule  # ‚ùå At module level in main file
```

‚úÖ Solution for main declare_logic.py: Import inside function
```python
# In logic/declare_logic.py (main file)
from database import models  # ‚úÖ At module level

def declare_logic():
    from logic_bank.logic_bank import Rule  # ‚úÖ Inside function in main file
```

‚úÖ Discovery files (logic_discovery/*.py) are SAFE with module-level imports:
```python
# In logic/logic_discovery/check_credit.py or supplier_selection.py
from logic_bank.logic_bank import Rule  # ‚úÖ Safe in discovery files
from logic_bank.exec_row_logic.logic_row import LogicRow  # ‚úÖ Safe in discovery files  
from database import models  # ‚úÖ Preferred pattern

def declare_logic():
    # Rules here
```

**ERROR 2: Auto-Discovery Structure Requirements**

‚ö†Ô∏è **IMPORTANT**: logic/logic_discovery/auto_discovery.py is AUTO-GENERATED by ApiLogicServer
   - It is ALREADY CORRECT in all new projects (handles recursion + skips __init__.py)
   - ‚ùå DO NOT modify auto_discovery.py
   - ‚úÖ DO create logic files in proper structure that auto-discovery will find

‚úÖ What auto_discovery.py does (already built-in):
- Recursively scans logic_discovery/ and all subdirectories
- Finds all .py files except auto_discovery.py and __init__.py
- Imports each file and calls declare_logic() function
- Works with nested directories like ai_requests/, validation/, etc.

‚úÖ Your responsibility (what Copilot generates):
```
logic/logic_discovery/
  check_credit.py              # Has declare_logic() function
  ai_requests/                 # Subdirectory
    __init__.py                # Empty file (makes it a package)
    supplier_selection.py      # Has declare_logic() function
```

‚ùå Common mistake: Putting logic in __init__.py
- auto_discovery.py skips __init__.py files (by design)
- Always create separate .py files with declare_logic() functions

**ERROR 3: Path Resolution for YAML Files**
‚ùå Problem: Path(__file__).parent creates relative path
```python
context_file = config_dir / 'ai_test_context.yaml'
if context_file.exists():  # ‚ùå May fail on relative paths
```

‚úÖ Solution: Use .resolve() for absolute paths
```python
current_file = Path(__file__).resolve()  # ‚úÖ Absolute path
project_root = current_file.parent.parent.parent.parent
context_file = project_root / 'config' / 'ai_test_context.yaml'
if context_file.exists():
    with open(str(context_file), 'r') as f:  # ‚úÖ Convert to string
```

**See docs/training/genai_logic_patterns.md for complete patterns.**

=============================================================================
ü§ñ AUTOMATIC DETECTION: When to Generate Probabilistic Logic
=============================================================================

WHEN YOU SEE these patterns in user's natural language prompt, AUTOMATICALLY 
create complete AI value computation infrastructure:

‚úÖ Trigger Phrases:
   - "use AI to select/choose..."
   - "optimize using AI..."
   - "AI should decide..."
   - "intelligently select based on..."
   - "considering [multiple factors] choose..."
   - "IF [condition] THEN use AI to [select/compute]..."

‚úÖ What to Generate AUTOMATICALLY (complete working implementation):
   
   0. **CRITICAL FILE STRUCTURE** - Modular architecture (production pattern):
      ```
      logic/
        logic_discovery/
          check_credit.py              # Deterministic rules + conditional event handler
          ai_requests/                 # ‚Üê SEPARATE DIRECTORY for AI handlers
            __init__.py                # Empty file for Python package
            supplier_selection.py      # AI-specific handler with compute_ai_value()
        system/                        # ‚Üê Reusable utilities
          __init__.py                  # Empty file
          ai_value_computation.py      # ‚Üê Reusable introspection utility
      ```
      
      ‚ö†Ô∏è **MANDATORY**: 
      - ‚úÖ DO create separate ai_requests/ directory for AI handlers
      - ‚úÖ DO create logic/system/ directory if it doesn't exist
      - ‚úÖ DO implement ai_value_computation.py using introspection pattern
      - ‚ùå DO NOT put everything in one file
      - ‚ùå DO NOT inline AI handler in check_credit.py
   
   1. **CRITICAL**: Check if sys_xxx_req table exists in DATABASE (not just models.py)
      - Run: sqlite3 database/db.sqlite ".tables" | grep -i sys_xxx_req
      - If table missing ‚Üí COPILOT MUST AUTOMATICALLY add model and run COMPLETE Alembic migration
      - ‚ùå WRONG: Assuming table exists because model is in models.py
      - ‚ùå WRONG: Telling user to run migration manually
      - ‚ùå WRONG: Running migration generation but not cleaning/applying it
      - ‚úÖ CORRECT: Verify database, add model, run migration, clean migration file, apply migration - ALL AUTOMATICALLY
   
   2. If table does NOT exist in database - DO ALL THESE STEPS:
      a. Add SysXxxReq model to database/models.py (include datetime import!)
      b. Add relationships to parent models (bidirectional)
      c. **RUN ALEMBIC MIGRATION AUTOMATICALLY** - Use run_in_terminal tool:
         ```bash
         cd database && alembic revision --autogenerate -m "Add SysXxxReq audit table"
         ```
         Then **COPILOT MUST AUTOMATICALLY**:
         - Read the generated migration file from database/alembic/versions/
         - Remove ALL ALTER TABLE operations (keep only CREATE TABLE sys_xxx_req)
         - Simplify downgrade() to just DROP TABLE
         - Save the cleaned migration file
         Then run:
         ```bash
         cd database && alembic upgrade head
         ```
      d. After migration completes, proceed to step 3
   
   3. After database table exists (either already there or just created):
      a. **Implement logic/system/ai_value_computation.py** if not present (see template)
      b. Create logic/logic_discovery/check_credit.py (deterministic rules + conditional handler)
      c. Create logic/logic_discovery/ai_requests/__init__.py (empty file)
      d. Create logic/logic_discovery/ai_requests/supplier_selection.py (AI handler with compute_ai_value())
      e. Create config/ai_test_context.yaml for reproducible testing
      f. Update ui/admin/admin.yaml with SysXxxReq resource
      g. ALL rule registrations MUST be inside declare_logic() function
      h. Utility files (ai_value_computation.py) have NO declare_logic() function

‚úÖ Example User Prompt:
   "Item unit_price: IF Product has suppliers, use AI to select optimal 
    supplier based on cost and lead time, ELSE copy from Product.unit_price"

‚úÖ You MUST Follow This Sequence:

STEP 1 - DATABASE VERIFICATION (MANDATORY):
   ```bash
   sqlite3 database/db.sqlite ".tables" | grep -i sys_supplier_req
   ```
   - If no output ‚Üí Table missing, need migration
   - If output shows table ‚Üí Proceed to STEP 3

STEP 2 - MODEL AND MIGRATION REQUIRED (if table missing):
   
   Add SysXxxReq model to database/models.py and run Alembic migration **AUTOMATICALLY**.
   
   **See "Common Patterns" section below for complete Alembic migration workflow** including:
   - Model structure with datetime import
   - Migration generation and AUTOMATIC cleaning (remove ALTER TABLE operations)
   - Common issues and solutions
   
   Quick steps **COPILOT MUST EXECUTE AUTOMATICALLY**:
   1. Add model class to database/models.py (include `import datetime`)
   2. Add bidirectional relationships to parent models
   3. Run: `cd database && alembic revision --autogenerate -m "Add SysXxxReq audit table"`
   4. **AUTOMATICALLY read and clean migration file**: Remove ALL ALTER TABLE operations, keep only CREATE TABLE
   5. **AUTOMATICALLY save cleaned migration file**
   6. Run: `cd database && alembic upgrade head`

STEP 3 - IMPLEMENT AI_VALUE_COMPUTATION.PY UTILITY (CRITICAL):
   
   ‚ö†Ô∏è **MANDATORY**: Before generating ANY AI logic, check if logic/system/ai_value_computation.py exists
   
   ```bash
   test -f logic/system/ai_value_computation.py && echo "EXISTS" || echo "MISSING"
   ```
   
   If MISSING, you MUST implement the introspection-based compute_ai_value() utility:
   
   **Required Capabilities**:
   - ‚úÖ Introspection-based candidate discovery via SQLAlchemy relationships
   - ‚úÖ Automatic field mapping from candidates to result columns
   - ‚úÖ OpenAI API integration with structured prompts
   - ‚úÖ YAML test context loading from config/ai_test_context.yaml
   - ‚úÖ Graceful fallback strategies (min/max/first) when no API key
   - ‚úÖ Type conversion (Decimal for money, int for IDs)
   - ‚úÖ Complete audit trail generation
   
   **Implementation Pattern**: See template_probabilistic_rules.py for reference implementation
   
   ‚úÖ Create logic/system/ directory if it doesn't exist
   ‚úÖ Create logic/system/__init__.py (empty file)
   ‚úÖ Implement compute_ai_value() function with introspection logic

STEP 4 - GENERATE LOGIC FILES (only after utility exists):
   
   a. Create logic/logic_discovery/check_credit.py
      - Deterministic rules (constraint, sums, formulas)
      - Only add Rule.count() if user explicitly requests it (e.g., "Product.count_suppliers = count of ProductSupplier")
      - Assumes Product.count_suppliers exists in schema if referenced in conditional logic
      - Null-safe constraint and formulas
      - Conditional event handler (ItemUnitPriceFromSupplier)
      - Conditional formula (preserves AI value or uses default)
   
   b. Create logic/logic_discovery/ai_requests/__init__.py (empty file)
   
   c. Create logic/logic_discovery/ai_requests/supplier_selection.py
      - Import compute_ai_value from logic.system.ai_value_computation
      - declare_logic() function with Rule.early_row_event registration
      - AI handler function that calls compute_ai_value()
      - Use relationship navigation: 'product.ProductSupplierList'
      - Specify optimize_for and fallback strategies
   
   d. Create config/ai_test_context.yaml
      - world_conditions for reproducible testing
      - Other context fields as needed
   
   e. Update ui/admin/admin.yaml
      - Add SysSupplierReq resource with all fields

This verification and file structure is MANDATORY - works for ANY project structure.

=============================================================================
REQUIRED FILE STRUCTURE: Modular Architecture (Production Pattern)
=============================================================================

**CRITICAL**: Always generate this EXACT file structure for probabilistic logic:

File 1: logic/logic_discovery/check_credit.py
```python
"""
Check Credit Logic - Declarative Rules with Probabilistic AI Supplier Selection

Natural Language Requirements:
[... copy user's requirements ...]

Generated: Auto-generated from natural language by GitHub Copilot
"""

from logic_bank.exec_row_logic.logic_row import LogicRow
from logic_bank.logic_bank import Rule
from database import models  # Preferred: consistent with PDL pattern


def declare_logic():
    """
    Declarative rules for Check Credit use case.
    
    Combines deterministic rules (sums, formulas, constraints) with 
    probabilistic AI value computation (supplier selection).
    """
    
    # Rule 1: Constraint with null-safety
    Rule.constraint(
        validate=models.Customer,
        as_condition=lambda row: row.balance is None or row.credit_limit is None or row.balance <= row.credit_limit,
        error_msg="Customer balance ({row.balance}) exceeds credit limit ({row.credit_limit})"
    )
    
    # Rule 2: Customer balance = sum of unshipped orders
    Rule.sum(
        derive=models.Customer.balance,
        as_sum_of=models.Order.amount_total,
        where=lambda row: row.date_shipped is None
    )
    
    # Rule 3: Order amount_total = sum of items
    Rule.sum(
        derive=models.Order.amount_total,
        as_sum_of=models.Item.amount
    )
    
    # Rule 4: Item amount = quantity * unit_price (null-safe)
    Rule.formula(
        derive=models.Item.amount,
        as_expression=lambda row: row.quantity * (row.unit_price or 0)
    )
    
    # Rule 5a: Early event - triggers AI if suppliers exist
    # NOTE: Assumes Product.count_suppliers exists in database schema
    #       If user specifies count rule, add it here
    Rule.early_row_event(
        on_class=models.Item,
        calling=lambda row, old_row, logic_row: ItemUnitPriceFromSupplier(row, logic_row)
    )
    
    # Rule 5b: Conditional formula - AI value or default
    # Uses Product.count_suppliers from database schema
    Rule.formula(
        derive=models.Item.unit_price,
        as_expression=lambda row: (
            row.product.unit_price if row.product.count_suppliers == 0
            else row.unit_price  # Preserve AI-set value
        )
    )


def ItemUnitPriceFromSupplier(item_row: models.Item, logic_row: LogicRow):
    """
    Conditional AI logic: IF product has suppliers THEN use AI ELSE skip.
    
    This creates SysSupplierReq using Request Pattern, which triggers
    the AI handler in ai_requests/supplier_selection.py
    """
    if not logic_row.is_inserted():
        return
    
    if item_row.product.count_suppliers == 0:
        logic_row.log(f"Item - Product has no suppliers, using default unit_price")
        return
    
    logic_row.log(f"Item - Product has {item_row.product.count_suppliers} suppliers, invoking AI")
    
    # Request Pattern
    supplier_req_logic_row = logic_row.new_logic_row(models.SysSupplierReq)
    supplier_req = supplier_req_logic_row.row
    
    supplier_req.item_id = item_row.id
    supplier_req.product_id = item_row.product_id
    
    supplier_req_logic_row.insert(reason="AI supplier selection request")
    
    # CRITICAL: Copy AI result
    item_row.unit_price = supplier_req.chosen_unit_price
    logic_row.log(f"Item - AI selected supplier, unit_price set to {item_row.unit_price}")
```

File 2: logic/logic_discovery/ai_requests/__init__.py
```python
# Empty file - makes this a Python package
```

File 3: logic/logic_discovery/ai_requests/supplier_selection.py
```python
"""
AI Supplier Selection Handler - Probabilistic Value Computation

Uses introspection-based compute_ai_value() utility to automatically:
- Navigate product.ProductSupplierList relationship
- Introspect all candidate fields
- Call OpenAI with structured prompt
- Map AI response to SysSupplierReq result columns
- Handle graceful fallback when no API key
- Store complete audit trail

This AI handler fires when SysSupplierReq is inserted via Request Pattern.
"""

from logic_bank.exec_row_logic.logic_row import LogicRow
from logic_bank.logic_bank import Rule
from database import models
from logic.system.ai_value_computation import compute_ai_value


def declare_logic():
    """
    Register AI supplier selection handler.
    
    Fires on SysSupplierReq insert, uses introspection-based utility
    to automatically discover candidates and compute optimal selection.
    """
    Rule.early_row_event(
        on_class=models.SysSupplierReq,
        calling=supplier_id_from_ai
    )


def supplier_id_from_ai(row: models.SysSupplierReq, old_row, logic_row: LogicRow):
    """
    AI selects optimal supplier based on cost, lead time, and world conditions.
    
    Uses compute_ai_value() utility which automatically:
    - Navigates row.product.ProductSupplierList to get candidates
    - Introspects ProductSupplier columns: supplier_id, unit_cost, lead_time_days
    - Introspects Supplier columns via relationship: name, region
    - Introspects SysSupplierReq result columns: chosen_supplier_id, chosen_unit_price
    - Maps AI response: chosen_supplier_id ‚Üê supplier_id, chosen_unit_price ‚Üê unit_cost
    - Loads world_conditions from config/ai_test_context.yaml
    - Calls OpenAI with structured prompt
    - Handles fallback: min:unit_cost when no API key
    - Stores complete audit trail
    """
    if not logic_row.is_inserted():
        return
    
    logic_row.log(f"SysSupplierReq - AI supplier selection starting")
    
    # Introspection-based AI value computation
    compute_ai_value(
        row=row,
        logic_row=logic_row,
        candidates='product.ProductSupplierList',
        optimize_for='fastest reliable delivery while keeping costs reasonable, considering world conditions like supply chain disruptions',
        fallback='min:unit_cost'
    )
    
    logic_row.log(f"SysSupplierReq - AI selection complete: supplier_id={row.chosen_supplier_id}, unit_price={row.chosen_unit_price}")
```

File 4: logic/system/__init__.py
```python
# Empty file - makes this a Python package
```

File 5: logic/system/ai_value_computation.py
**CRITICAL**: Implement this utility with introspection-based pattern:
  
This utility provides introspection-based AI value computation with:
- Automatic candidate discovery via SQLAlchemy relationships
- Dynamic column introspection for field mapping
- OpenAI API integration with structured prompts
- Graceful fallback strategies (min/max/first)
- YAML test context loading from config/ai_test_context.yaml
- Complete audit trail generation with type conversion

See template_probabilistic_rules.py for implementation reference.

File 6: config/ai_test_context.yaml
```yaml
# AI Test Context - Enables reproducible AI testing
world_conditions: 'ship aground in Suez Canal'
market_conditions: null
traffic_conditions: null
weather_conditions: null
```

‚ö†Ô∏è **WHY THIS STRUCTURE IS MANDATORY**:
1. **Separation of Concerns**: Business logic separate from AI handlers
2. **Reusability**: ai_value_computation.py works for ANY AI selection scenario
3. **Testability**: Test context YAML enables reproducible testing
4. **Maintainability**: Clear file organization, easy to find and update
5. **Discovery System**: auto_discovery.py finds and loads all logic files automatically

=============================================================================
REQUIRED PATTERN: Conditional Formula with AI
=============================================================================

When user says "IF product has suppliers THEN use AI ELSE use default", use this exact pattern:

NOTE: This pattern ASSUMES Product.count_suppliers exists in the database schema.
      Only add Rule.count() if user explicitly requests it in natural language.

# Step 1: Register early event handler (fires BEFORE formula)
Rule.early_row_event(
    on_class=models.Item,
    calling=lambda row, old_row, logic_row: ItemUnitPriceFromSupplier(row, logic_row)
)

# Step 2: Formula that preserves AI-set value or uses default
# Assumes Product.count_suppliers exists in schema
Rule.formula(
    derive=models.Item.unit_price,
    as_expression=lambda row: (
        row.product.unit_price if row.product.count_suppliers == 0
        else row.unit_price  # Preserve value set by event handler
    )
)

# Step 3: Event handler that creates request and copies AI result
def ItemUnitPriceFromSupplier(item_row: models.Item, logic_row: LogicRow):
    # Assumes Product.count_suppliers exists in database schema
    if not logic_row.is_inserted() or item_row.product.count_suppliers == 0:
        return
    
    # Create request using new_logic_row (pass CLASS not instance)
    supplier_req_logic_row = logic_row.new_logic_row(models.SysSupplierReq)
    supplier_req = supplier_req_logic_row.row  # Get the instance AFTER creation
    
    # Set request context
    supplier_req.item_id = item_row.id
    supplier_req.product_id = item_row.product_id
    
    # Insert triggers AI handler
    supplier_req_logic_row.insert(reason="AI supplier selection request")
    
    # CRITICAL: Copy AI result to target row
    item_row.unit_price = supplier_req.chosen_unit_price

CRITICAL - Must copy AI result to target row:
‚úÖ Pass CLASS to new_logic_row: new_logic_row(models.SysSupplierReq)
‚úÖ Get instance from .row property: supplier_req = supplier_req_logic_row.row
‚úÖ Insert using logic_row: supplier_req_logic_row.insert()
‚úÖ **Copy AI result to target: item_row.unit_price = supplier_req.chosen_unit_price**

Why: AI populates request table (SysSupplierReq.chosen_unit_price), but target table 
(Item.unit_price) is separate. No automatic propagation - event handler MUST copy explicitly.

# For Request Pattern details, see docs/training/logic_bank_patterns.prompt PATTERN 3

# For Rule API syntax, see docs/training/logic_bank_patterns.prompt PATTERN 4
# For Event handler signatures, see docs/training/logic_bank_patterns.prompt PATTERN 1
# For Logging patterns, see docs/training/logic_bank_patterns.prompt PATTERN 2

=============================================================================
AI-SPECIFIC: compute_ai_value() Type Handling
=============================================================================

The compute_ai_value() utility automatically handles type conversion for AI responses.

NOTE: For general type handling patterns in LogicBank, see docs/training/logic_bank_patterns.prompt PATTERN 7.

This utility automatically converts AI response values to correct database types:
- Foreign keys (_id fields) ‚Üí int
- Monetary fields (_price, _cost, _amount) ‚Üí Decimal
- Other numeric fields ‚Üí float or int based on column type

You don't need manual type conversion when using compute_ai_value().

---

```plaintext
Here is the API for LogicBank Probabilistic Rules (AI Value Computation):

Translate user prompts about AI-computed values into probabilistic rules using the introspection-based pattern.

This extends the deterministic LogicBank rules with probabilistic value computation capabilities.
Use this when computing values requires contextual reasoning that cannot be expressed as simple formulas.

---

Implementation Pattern: Introspection-Based Utility

**CRITICAL**: ALWAYS use the logic/system/ai_value_computation.py utility for AI value computation.

This utility provides introspection-based AI value computation and should be implemented
following the pattern in template_probabilistic_rules.py if not already present.

Basic usage pattern (in separate AI handler file):

```python
# File: logic/logic_discovery/ai_requests/supplier_selection.py

from logic.system.ai_value_computation import compute_ai_value
from logic_bank.exec_row_logic.logic_row import LogicRow
from logic_bank.logic_bank import Rule
from database import models


def declare_logic():
    """Register AI handler on SysSupplierReq insert."""
    Rule.early_row_event(on_class=models.SysSupplierReq, calling=supplier_id_from_ai)


def supplier_id_from_ai(row: models.SysSupplierReq, old_row, logic_row: LogicRow):
    """AI computes optimal supplier selection."""
    if not logic_row.is_inserted():
        return
    
    logic_row.log("AI supplier selection starting")
    
    compute_ai_value(
        row=row,                              # Request table row (SysSupplierReq, etc.)
        logic_row=logic_row,                  # For logging and DB operations
        candidates='product.ProductSupplierList',  # Relationship path to candidates
        optimize_for='fastest reliable delivery while keeping costs reasonable',
        fallback='min:unit_cost'             # Strategy when no API key
    )
    
    logic_row.log(f"AI selection complete: supplier_id={row.chosen_supplier_id}")
```

What compute_ai_value() does automatically (introspection-based pattern):
- ‚úÖ Discovers candidate objects via relationship navigation (row.product.ProductSupplierList)
- ‚úÖ Introspects ALL candidate fields via SQLAlchemy (supplier_id, supplier_name, unit_cost, lead_time_days, region, etc.)
- ‚úÖ Introspects related entity attributes (supplier.name, supplier.region)
- ‚úÖ Introspects request table chosen_* columns (chosen_supplier_id, chosen_unit_price)
- ‚úÖ Maps AI response to result columns (chosen_supplier_id ‚Üê supplier_id, chosen_unit_price ‚Üê unit_cost)
- ‚úÖ Handles naming variations (unit_price ‚Üî unit_cost)
- ‚úÖ Loads test context from config/ai_test_context.yaml
- ‚úÖ Calls OpenAI API with structured JSON prompt
- ‚úÖ Handles fallback when no API key (min/max/first strategies)
- ‚úÖ Converts types properly (Decimal for money, int for IDs)
- ‚úÖ Stores complete audit trail (request JSON, reason, created_on)

**Why NOT write AI logic inline:**
‚ùå Manual field serialization (error-prone)
‚ùå Hardcoded field names (breaks when schema changes)
‚ùå No introspection (miss new columns)
‚ùå Duplicate OpenAI integration code
‚ùå No test context support
‚ùå Inconsistent audit trails

‚úÖ compute_ai_value() handles ALL of this automatically via introspection
        
        Example 1: Supplier Selection
            Prompt:
                Choose the best supplier for this item considering cost, lead time, 
                and current world conditions. Optimize for fastest delivery when 
                disruptions are present. [store in SysSupplierReq]
            
            Response (user code in logic/logic_discovery/check_credit.py):
                from logic.system.ai_value_computation import compute_ai_value
                
                def supplier_id_from_ai(row: models.SysSupplierReq, old_row, logic_row: LogicRow):
                    """AI selects optimal supplier based on cost, lead time, and world conditions."""
                    if not logic_row.is_inserted():
                        return
                    
                    compute_ai_value(
                        row=row,
                        logic_row=logic_row,
                        candidates='product.ProductSupplierList',
                        optimize_for='fastest reliable delivery while keeping costs reasonable',
                        fallback='min:unit_cost'
                    )
                
                Rule.early_row_event(on_class=models.SysSupplierReq, calling=supplier_id_from_ai)
            
            What gets introspected automatically:
                - Candidates: supplier_id, supplier_name, unit_cost, lead_time_days, region
                - Results: chosen_supplier_id (‚Üê supplier_id), chosen_unit_price (‚Üê unit_cost)
                - Test context: world_conditions from config/ai_test_context.yaml

        Example 2: Dynamic Pricing
            Prompt:
                Set the optimal price for this product considering competitor prices,
                current inventory levels, and demand forecast. Optimize for profit 
                while maintaining competitive position. Store reasoning in pricing_reason.
            
            Response:
                Rule.ai_decision(
                    derive=Product.current_price,
                    from_candidates=lambda row: [
                        {'price': p} 
                        for p in range(int(row.cost * 1.1), int(row.cost * 2.0), 5)
                    ],
                    optimize_for=['profit_margin', 'competitive_position'],
                    considering={
                        'competitor_avg': row.competitor_avg_price,
                        'inventory_level': row.stock_quantity,
                        'demand_trend': row.demand_forecast
                    },
                    reasoning_to=Product.pricing_reason
                )

        Example 3: Route Optimization
            Prompt:
                Choose the best delivery route considering current traffic, weather 
                conditions, and delivery urgency. Optimize for fastest delivery time.
                Store the reasoning in route_reason.
            
            Response:
                Rule.ai_decision(
                    derive=Delivery.chosen_route_id,
                    from_candidates=lambda row: [
                        {'id': r.id, 
                         'distance_miles': r.distance_miles, 
                         'typical_minutes': r.typical_minutes,
                         'toll_cost': r.toll_cost}
                        for r in row.destination.AvailableRouteList
                    ],
                    optimize_for=['delivery_time', 'fuel_cost'],
                    considering={
                        'traffic': 'current heavy on I-95',
                        'weather': row.weather_conditions,
                        'priority': row.priority_level
                    },
                    reasoning_to=Delivery.route_reason
                )

        Example 4: Staff Assignment
            Prompt:
                Assign the best qualified staff member to this project considering
                their skills, current workload, and project requirements. Optimize
                for project success probability. Store reasoning in assignment_reason.
            
            Response:
                Rule.ai_decision(
                    derive=Project.assigned_staff_id,
                    from_candidates=lambda row: [
                        {'id': s.id,
                         'skill_match': s.skill_score_for_project(row),
                         'availability': s.available_hours,
                         'experience_years': s.years_experience}
                        for s in StaffMember.query.filter_by(available=True).all()
                    ],
                    optimize_for=['skill_match', 'availability'],
                    considering={
                        'project_complexity': row.complexity_rating,
                        'deadline': row.due_date,
                        'budget': row.budget_constraint
                    },
                    reasoning_to=Project.assignment_reason
                )

        Args:
            derive: <class.attribute> where the chosen candidate ID (or value) will be stored
            from_candidates: lambda returning list of dicts. Each dict must have:
                - 'id' or the value being selected (for derive column)
                - factor names matching those in optimize_for
                - any other contextual attributes AI should consider
            optimize_for: list of factor names to optimize (in priority order)
                First item is highest priority. AI will balance all factors.
            considering: dict of contextual conditions for AI to reason about
                Keys are condition names, values are either:
                - String literals: 'Suez Canal blocked'
                - Row attribute references: row.weather_conditions
                - Computed values: row.calculate_urgency()
            reasoning_to: optional <class.attribute> where AI explanation will be stored
                Use this for governance, audit trails, and explainability
            request_to: optional <class.attribute> where the full AI request will be stored
                Includes the complete prompt sent to the LLM for reproducibility
            model: OpenAI model to use (default: gpt-4o-2024-08-06)
                Can specify different models for different cost/quality tradeoffs
        
        Notes:
            - AI decisions are only invoked when the rule's derived column is accessed
            - Automatically inherits transaction context and rollback behavior
            - Falls back gracefully: if API unavailable, selects first candidate
            - Response format is always structured JSON for reliable parsing
            - Full audit trail stored in request_to and reasoning_to columns
        """
        return AiDecision(derive=derive,
                         from_candidates=from_candidates,
                         optimize_for=optimize_for,
                         considering=considering,
                         reasoning_to=reasoning_to,
                         request_to=request_to,
                         model=model)


Integration with Deterministic Rules:

Probabilistic rules work seamlessly with deterministic rules in the same logic flow:

    Prompt:
        When an item is added to an order, choose the best supplier using AI,
        then calculate the item amount as quantity * unit_price,
        then update the order total as sum of item amounts,
        then update the customer balance as sum of unshipped order totals,
        then validate that customer balance does not exceed credit limit.

    Response:
        # Probabilistic: AI chooses supplier and sets unit_price
        Rule.ai_decision(
            derive=SysSupplierReq.chosen_supplier_id,
            from_candidates=lambda row: [
                {'id': ps.supplier_id, 'cost': float(ps.unit_cost), 
                 'lead_time_days': ps.lead_time_days}
                for ps in row.product.ProductSupplierList
            ],
            optimize_for=['lead_time_days', 'cost'],
            considering={'world_conditions': 'Suez Canal blocked'},
            reasoning_to=SysSupplierReq.reason
        )
        
        # Deterministic: calculations cascade automatically
        Rule.formula(derive=Item.amount, 
                    as_expression=lambda row: row.quantity * row.unit_price)
        Rule.sum(derive=Order.amount_total, as_sum_of=Item.amount)
        Rule.sum(derive=Customer.balance, as_sum_of=Order.amount_total,
                where=lambda row: row.date_shipped is None)
        
        # Deterministic: validation (guardrail for AI decision)
        Rule.constraint(validate=Customer,
                       as_condition=lambda row: row.balance <= row.credit_limit,
                       error_msg="Customer balance exceeds credit limit")

The key insight: AI makes the probabilistic decision (supplier choice), 
then deterministic rules cascade and validate automatically.
If AI's choice violates constraints, transaction rolls back - no special code needed.


Common Patterns:

1. Request Pattern (Required):
   ALWAYS create a SysXxxReq table to capture AI requests/responses for audit.
   This is mandatory for governance, explainability, and debugging.
   
   Convention - Sys{Domain}Req table structure:
   - Name: Sys{Domain}Req (e.g., SysSupplierReq, SysPricingReq, SysRouteReq)
   - chosen_{domain}_id: FK to selected entity (e.g., chosen_supplier_id)
   - request: String(2000) - full AI prompt sent
   - reason: String(500) - AI's explanation
   - created_on: DateTime - timestamp
   - Context FKs: Links to triggering entities (e.g., item_id, product_id)
   
   Table Generation Logic:
   a) Check if Sys{Domain}Req exists in database/models.py
   b) If exists and matches convention ‚Üí Use it (generate logic only)
   c) If exists but wrong structure ‚Üí ERROR with clear message (fail fast)
   d) If doesn't exist ‚Üí Create model + generate Alembic migration
   e) Add relationships to other models (Item, Product, Supplier)
   
   =============================================================================
   CRITICAL: Production-Ready Database Schema Changes via Alembic
   =============================================================================
   
   When Sys{Domain}Req model doesn't exist, use this STANDARD workflow:
   
   ‚úÖ Step 1: Add SQLAlchemy model to database/models.py with relationships:
   
   class SysSupplierReq(Base):
       __tablename__ = "sys_supplier_req"
       _s_collection_name = 'SysSupplierReq'
       
       id = Column(Integer, primary_key=True)
       item_id = Column(Integer, ForeignKey("item.id"), index=True, nullable=True)
       product_id = Column(Integer, ForeignKey("product.id"), index=True, nullable=False)
       chosen_supplier_id = Column(Integer, ForeignKey("supplier.id"))
       chosen_unit_price : DECIMAL = Column(DECIMAL)
       request = Column(String(2000))
       reason = Column(String(500))
       created_on = Column(DateTime, default=datetime.datetime.utcnow, nullable=False)
       
       # parent relationships
       item : Mapped["Item"] = relationship(back_populates="SysSupplierReqList")
       product : Mapped["Product"] = relationship(back_populates="SysSupplierReqList")
       chosen_supplier : Mapped["Supplier"] = relationship()
   
   ‚úÖ Step 2: Add child relationships to parent models:
   
   # In Product class:
   SysSupplierReqList : Mapped[List["SysSupplierReq"]] = relationship(back_populates="product")
   
   # In Item class:
   SysSupplierReqList : Mapped[List["SysSupplierReq"]] = relationship(back_populates="item")
   
   ‚úÖ Step 3: Generate Alembic migration (DO NOT manually create SQL):
   
   **COPILOT MUST RUN THIS AUTOMATICALLY:**
   ```bash
   cd database
   alembic revision --autogenerate -m "Add sys_supplier_req table for AI audit"
   ```
   
   This creates: database/alembic/versions/xxxxx_add_sys_supplier_req_table.py
   
   ‚ö†Ô∏è CRITICAL: **COPILOT MUST AUTOMATICALLY** clean the generated migration file:
   
   # Alembic --autogenerate detects ALL differences between models.py and database
   # COPILOT must automatically read and edit the generated file:
   
   1. Read the generated migration file from database/alembic/versions/
   2. KEEP: CREATE TABLE sys_supplier_req statement
   3. REMOVE: Any ALTER TABLE on existing tables (unrelated changes)
   4. SIMPLIFY downgrade(): Just DROP TABLE sys_supplier_req
   5. Save the cleaned migration file
   
   Example cleaned migration:
   
   ```python
   def upgrade():
       op.create_table('sys_supplier_req',
           sa.Column('id', sa.Integer(), nullable=False),
           sa.Column('item_id', sa.Integer(), nullable=True),
           sa.Column('product_id', sa.Integer(), nullable=False),
           sa.Column('chosen_supplier_id', sa.Integer(), nullable=True),
           sa.Column('chosen_unit_price', sa.DECIMAL(), nullable=True),
           sa.Column('request', sa.String(length=2000), nullable=True),
           sa.Column('reason', sa.String(length=500), nullable=True),
           sa.Column('created_on', sa.DateTime(), nullable=False),
           sa.ForeignKeyConstraint(['chosen_supplier_id'], ['supplier.id']),
           sa.ForeignKeyConstraint(['item_id'], ['item.id']),
           sa.ForeignKeyConstraint(['product_id'], ['product.id']),
           sa.PrimaryKeyConstraint('id')
       )
   
   def downgrade():
       op.drop_table('sys_supplier_req')
   ```
   
   ‚úÖ Step 4: Apply migration to update database schema:
   
   **COPILOT MUST RUN THIS AUTOMATICALLY:**
   ```bash
   cd database
   alembic upgrade head
   ```
   
   This is PRODUCTION-READY:
   - ‚úÖ Version controlled (migration file in git)
   - ‚úÖ Reversible (alembic downgrade)
   - ‚úÖ Team-friendly (others run same migration)
   - ‚úÖ Staged deployment (dev ‚Üí test ‚Üí prod)
   - ‚úÖ Audit trail (history of schema changes)
   
   ‚úÖ Step 5: Update ui/admin/admin.yaml to add SysSupplierReq resource
   
   ‚úÖ Step 6: User restarts server (to load new model and migration)
   
   
=============================================================================
COMPLETE AUTOMATED WORKFLOW EXAMPLE (What Copilot Must Do)
=============================================================================

When user provides natural language with AI selection requirement:

‚úÖ **STEP 1**: Add model to database/models.py (with datetime import)
‚úÖ **STEP 2**: Add relationships to parent models
‚úÖ **STEP 3**: Run terminal command: `cd database && alembic revision --autogenerate -m "Add sys_supplier_req"`
‚úÖ **STEP 4**: Automatically find the new migration file in database/alembic/versions/
‚úÖ **STEP 5**: Automatically read the migration file
‚úÖ **STEP 6**: Automatically clean it: remove ALTER TABLE, keep CREATE TABLE, simplify downgrade()
‚úÖ **STEP 7**: Automatically save the cleaned migration file
‚úÖ **STEP 8**: Run terminal command: `cd database && alembic upgrade head`
‚úÖ **STEP 9**: Continue with logic file generation (check_credit.py, etc.)

**NO USER INTERVENTION REQUIRED** - Copilot handles the entire migration workflow automatically.
   
   =============================================================================
   Common Alembic Issues & Solutions
   =============================================================================
   
   Issue: "Target database is not up to date"
   Solution: Database schema doesn't match Alembic's tracking. Run:
     ```bash
     alembic stamp head
     ```
     This tells Alembic "database is current" and allows new migrations.
   
   Issue: "Table already exists"
   Solution: Database has tables but Alembic history is empty. Either:
     a) Use `alembic stamp head` to mark current state
     b) Drop conflicting tables and re-run migration
   
   Issue: "No config file 'alembic.ini' found"
   Solution: Must run alembic commands from database/ directory:
     ```bash
     cd database
     alembic revision --autogenerate -m "message"
     ```
   
   Issue: "python: command not found"
   Solution: Activate virtual environment first:
     ```bash
     source venv/bin/activate  # Linux/Mac
     venv\Scripts\activate     # Windows
     ```
   
   Why NOT Raw SQL?
   ‚ùå Raw SQL (CREATE TABLE...) is demo/testing only - NOT production-ready
   ‚ùå No version control
   ‚ùå No rollback capability
   ‚ùå Team members must manually sync
   ‚ùå No deployment history
   
   ‚úÖ Alembic migrations are the STANDARD approach for any real project
   
   Error Handling for Mismatched Tables:
   If SysXxxReq exists but doesn't match convention, FAIL with:
   
   "Error: Cannot implement AI logic. SysXxxReq table exists but doesn't match required convention.
   
   Expected fields: chosen_xxx_id, request, reason, created_on
   Found fields: [list actual fields]
   
   Resolution options:
   1. Rename existing table fields to match convention
   2. Specify different table name: [store in MyCustomAudit]
   3. Drop existing SysXxxReq table
   
   Please fix and re-run your request."
   
   Benefits: Complete audit trail, governance, debugging, reproducibility

2. Test Context from YAML:
   Support testing and demos via config/ai_test_context.yaml:
   
   # config/ai_test_context.yaml
   world_conditions: 'ship aground in Suez Canal'
   market_conditions: null
   traffic_conditions: null
   weather_conditions: null
   
   Usage in generated code:
   import yaml
   from pathlib import Path
   
   world_conditions = 'normal operations'
   try:
       config_dir = Path(__file__).parent.parent.parent / 'config'
       context_file = config_dir / 'ai_test_context.yaml'
       if context_file.exists():
           with open(context_file, 'r') as f:
               test_context = yaml.safe_load(f)
               world_conditions = test_context.get('world_conditions') or 'normal operations'
   except Exception as e:
       app_logger.debug(f"Could not load AI test context: {e}")
   
   This enables:
   - Reproducible testing
   - Demo scenarios without code changes
   - Version-controlled test conditions
   - Easy toggle between test/production
   - Clean separation from application config

3. Fallback Strategy (API Key Missing):
   Always provide graceful degradation when OpenAI unavailable.
   
   Infer fallback from user's optimization criteria:
   - "optimize for cost" ‚Üí fallback to min(candidates, key=lambda c: c['cost'])
   - "optimize for speed/time" ‚Üí fallback to min(candidates, key=lambda c: c['lead_time'])
   - "optimize for reliability" ‚Üí fallback to max(candidates, key=lambda c: c['rating'])
   - No optimization specified ‚Üí fallback to candidates[0] (first available)
   
   Generated pattern:
   api_key = os.getenv("APILOGICSERVER_CHATGPT_APIKEY")
   if not api_key:
       reasoning = "Fallback: no API key available, using [inferred strategy]"
       chosen = [apply inferred fallback logic]
       row.reason = reasoning
       return chosen
   
   Store fallback reasoning in audit trail for transparency.

4. Scope Validation:
   Probabilistic rules are for VALUE COMPUTATION and SELECTION.
   
   Valid patterns:
   ‚úÖ "choose/select X from Y based on Z"
   ‚úÖ "compute/calculate/set X considering Y"
   ‚úÖ "determine X by evaluating Y options"
   
   Invalid patterns (MUST ERROR):
   ‚ùå "ensure X is Y" (enforcement, not computation)
   ‚ùå "make X happen" (action, not decision)
   ‚ùå "predict X" (without selection context)
   ‚ùå "guarantee X" (too vague/subjective)
   
   Error for invalid requests:
   "Error: Cannot implement '[requirement]'
   
   Reason: This is not a computable value or selection decision.
   
   Probabilistic rules can:
   ‚úÖ Select from concrete candidates (suppliers, routes, prices)
   ‚úÖ Choose based on measurable factors (cost, time, distance)
   ‚úÖ Compute specific values (price, quantity, score)
   
   They cannot:
   ‚ùå Ensure subjective states ('happy', 'satisfied')
   ‚ùå Make open-ended predictions without candidates
   ‚ùå Enforce unmeasurable qualities
   
   Please reformulate as a selection or computation."


Error Handling:

The Rule.ai_decision automatically handles:
- Missing API key: falls back to first candidate
- API timeout: falls back with logged warning  
- Invalid JSON response: retries once, then fallback
- No candidates provided: raises clear error
- Constraint violations: normal LogicBank rollback

You do NOT need to write error handling code - it's built into the rule.


IMPLEMENTATION REFERENCE:

The Rule.ai_value() API is not yet implemented in LogicBank.
Until it is, use the MANUAL PATTERN from the template file.

See: docs/training/template_probabilistic_rules.py

This template shows the complete working pattern with:
- Request Pattern (SysXxxReq audit table)
- Conditional logic (when to invoke AI)
- AI event handler (OpenAI integration)
- Graceful fallbacks (no API key handling)
- Test context loading (from YAML)
- Error handling
- Complete audit trail

Copy relevant sections from the template and adapt to your specific use case.

The template is structured in 4 clear sections:
1. Deterministic Guardrails (constraints)
2. Deterministic Calculations (sums, formulas)
3. Conditional Logic (decides when AI runs)
4. AI Event Handler (probabilistic decision)

Each section has detailed comments explaining the pattern and showing working code.
```
